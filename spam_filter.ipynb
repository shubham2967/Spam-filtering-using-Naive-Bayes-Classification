{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "outputs": [],
   "metadata": {
    "colab_type": "text",
    "id": "EAdECfOXX_Gv"
   },
   "source": [
    "<h1>Introduction</h1>\n",
    "<p>The task is to implement a relatively simple spam filter.  We will use a small data set to train and test the model</p> \n",
    "    \n",
    "<p>The messages within the datasets have already been classified as ham (not spam), or spam. </p> \n",
    "\n",
    "<p>The implementation of a Naïve Bayes Spam Filter is relatively straight forward using scikit-learn, however, this library hides the implementation details (and many solutions are available on the Internet).  Therefore, we are not going to use it.</p> \n",
    "<p>Bayes Theorem can give us the probability that a message is spam S for a given event E</p>\n",
    "\n",
    "<h1>\n",
    "$P\\left(S\\middle|\\ E\\right)=\\frac{P\\left(E\\middle|\\ S\\right)P\\left(S\\right)}{P\\left(E\\middle|\\ S\\right)P\\left(S\\right)+P\\left(E|\\lnot S\\right)P\\left(\\lnot S\\right)}$\n",
    "</h1>\n",
    "\n",
    "<p>Where:</p>\n",
    "<p>$P\\left(S\\middle|\\ E\\right)$, the probability that the message is spam given the event occurred.</p>\n",
    "<p>$P\\left(S\\right)$, the prior probability that a message is spam.</p>\n",
    "<p>$P\\left(\\lnot S\\right)$, the prior probability that a message is not spam.  </p>\n",
    "\n",
    "Note:  $P\\left(S\\right)$ and $P\\left(\\lnot S\\right)$ are prior values, or prior beliefs.  This value could be calculated using the number of spam and number of ham classifications in the data set.  we could also use arbitrary values, for example; we could assume that of all email messages sent, 80% of them are spam and 20% of them are not spam.  The success of the filter depends on the prior values.\n",
    "\n",
    "<p>$P\\left(E\\middle|\\ S\\right)$, the probability that event E occurs in a spam emails.</p>\n",
    "\n",
    "<p>$P\\left(E|\\lnot S\\right)$, the probability that event E occurs in non-spam emails.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ep6k47M5X_Gw"
   },
   "source": [
    "<h3>1.  Read the dataset into a dataframe and explore</h3>\n",
    "<p>Starting by importing pandas and read the dataset into a DataFrame named df.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-05T00:46:28.056383Z",
     "start_time": "2023-05-05T00:46:28.016971Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "vq43eDWcX_Gx"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spam</td>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spam</td>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ham</td>\n",
       "      <td>I'm gonna be home soon and i don't want to tal...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>spam</td>\n",
       "      <td>SIX chances to win CASH! From 100 to 20,000 po...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>spam</td>\n",
       "      <td>URGENT! You have won a 1 week FREE membership ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to tha...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>spam</td>\n",
       "      <td>XXXMobileMovieClub: To use your credit, click ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ham</td>\n",
       "      <td>Oh k...i'm watching here:)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ham</td>\n",
       "      <td>Eh u remember how 2 spell his name... Yes i di...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ham</td>\n",
       "      <td>Fine if thatåÕs the way u feel. ThatåÕs the wa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>spam</td>\n",
       "      <td>England v Macedonia - dont miss the goals/team...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      v1                                                 v2 Unnamed: 2  \\\n",
       "0    ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1    ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3    ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4    ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "5   spam  FreeMsg Hey there darling it's been 3 week's n...        NaN   \n",
       "6    ham  Even my brother is not like to speak with me. ...        NaN   \n",
       "7    ham  As per your request 'Melle Melle (Oru Minnamin...        NaN   \n",
       "8   spam  WINNER!! As a valued network customer you have...        NaN   \n",
       "9   spam  Had your mobile 11 months or more? U R entitle...        NaN   \n",
       "10   ham  I'm gonna be home soon and i don't want to tal...        NaN   \n",
       "11  spam  SIX chances to win CASH! From 100 to 20,000 po...        NaN   \n",
       "12  spam  URGENT! You have won a 1 week FREE membership ...        NaN   \n",
       "13   ham  I've been searching for the right words to tha...        NaN   \n",
       "14   ham                I HAVE A DATE ON SUNDAY WITH WILL!!        NaN   \n",
       "15  spam  XXXMobileMovieClub: To use your credit, click ...        NaN   \n",
       "16   ham                         Oh k...i'm watching here:)        NaN   \n",
       "17   ham  Eh u remember how 2 spell his name... Yes i di...        NaN   \n",
       "18   ham  Fine if thatåÕs the way u feel. ThatåÕs the wa...        NaN   \n",
       "19  spam  England v Macedonia - dont miss the goals/team...        NaN   \n",
       "\n",
       "   Unnamed: 3 Unnamed: 4  \n",
       "0         NaN        NaN  \n",
       "1         NaN        NaN  \n",
       "2         NaN        NaN  \n",
       "3         NaN        NaN  \n",
       "4         NaN        NaN  \n",
       "5         NaN        NaN  \n",
       "6         NaN        NaN  \n",
       "7         NaN        NaN  \n",
       "8         NaN        NaN  \n",
       "9         NaN        NaN  \n",
       "10        NaN        NaN  \n",
       "11        NaN        NaN  \n",
       "12        NaN        NaN  \n",
       "13        NaN        NaN  \n",
       "14        NaN        NaN  \n",
       "15        NaN        NaN  \n",
       "16        NaN        NaN  \n",
       "17        NaN        NaN  \n",
       "18        NaN        NaN  \n",
       "19        NaN        NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import string\n",
    "\n",
    "df = pd.read_csv(\"spam.csv\", encoding= \"latin-1\")\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W1HwlygYX_G2"
   },
   "source": [
    "<h3>2. Clean the data</h3>\n",
    "<p>We are only interested in words, so cleaning the data so that all punctuations are removed.  We should be left with a dataset that only contains alpha characters (including spaces).  We should also ensure all the words are lowercase.  Storing the cleaned data into a DataFrame named clean.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-05T00:46:29.485935Z",
     "start_time": "2023-05-05T00:46:29.451631Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "JqppPCO_X_G3"
   },
   "outputs": [],
   "source": [
    "clean = df.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"] , axis = 1)\n",
    "clean = clean.rename(columns = {\"v1\": \"Category\",\"v2\": \"Message\"})\n",
    "clean[\"Message\"] = clean[\"Message\"].str.lower()\n",
    "clean['Message'] = clean['Message'].str.replace('[{}]'.format(string.punctuation), '', regex=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CLTcpAMvX_G8"
   },
   "source": [
    "<h3>3. Split the Data</h3>\n",
    "<p>Splitting the data into two random samples, one for training the model and the other for testing the model.  Creating DataFrames named train_data and test_data.  The train_data DataFrame should contain 75% of the data and the test_data DataFrame the remaining 25%.<p>  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-05T00:46:30.675933Z",
     "start_time": "2023-05-05T00:46:30.653978Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "fgKwaz1yX_G9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4179</th>\n",
       "      <td>ham</td>\n",
       "      <td>swhrt how u deyhope ur ok tot about u 2daylove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4180</th>\n",
       "      <td>ham</td>\n",
       "      <td>ok da i already planned i wil pick you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4181</th>\n",
       "      <td>spam</td>\n",
       "      <td>urgent please call 0906346330 your abta compli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4182</th>\n",
       "      <td>ham</td>\n",
       "      <td>sorry ill call later in meeting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4183</th>\n",
       "      <td>ham</td>\n",
       "      <td>i just really need shit before tomorrow and i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>this is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>will ì b going to esplanade fr home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>pity  was in mood for that soany other suggest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>the guy did some bitching but i acted like id ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>rofl its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1393 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Category                                            Message\n",
       "4179      ham  swhrt how u deyhope ur ok tot about u 2daylove...\n",
       "4180      ham             ok da i already planned i wil pick you\n",
       "4181     spam  urgent please call 0906346330 your abta compli...\n",
       "4182      ham                    sorry ill call later in meeting\n",
       "4183      ham  i just really need shit before tomorrow and i ...\n",
       "...       ...                                                ...\n",
       "5567     spam  this is the 2nd time we have tried 2 contact u...\n",
       "5568      ham                will ì b going to esplanade fr home\n",
       "5569      ham  pity  was in mood for that soany other suggest...\n",
       "5570      ham  the guy did some bitching but i acted like id ...\n",
       "5571      ham                          rofl its true to its name\n",
       "\n",
       "[1393 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = 0.75*(clean.shape[0])\n",
    "train_data = clean.iloc[:int(data)]\n",
    "test_data = clean.iloc[int(data):]\n",
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EU8azvVgX_HB"
   },
   "source": [
    "<h3>4. Create a Word Frequency DataFrame</h3>\n",
    "<p>Creating a new DataFrame named word_freq that contains each word with the number of times it appears in a spam and a ham message.  We should use the train_data not the test_data.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-05T00:46:32.183775Z",
     "start_time": "2023-05-05T00:46:32.057801Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "cJxL5N2BX_HC",
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>#Spam</th>\n",
       "      <th>#Ham</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3496</th>\n",
       "      <td>have</td>\n",
       "      <td>102.0</td>\n",
       "      <td>338.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Word  #Spam   #Ham\n",
       "3496  have  102.0  338.0"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_messages = \" \" + train_data[train_data['Category'] == 'spam']['Message']  + \" \"\n",
    "ham_messages = \" \" + train_data[train_data['Category'] == 'ham']['Message']  + \" \"\n",
    "\n",
    "spam_words = spam_messages.str.cat(sep=' ').split()\n",
    "ham_words = ham_messages.str.cat(sep=' ').split()\n",
    "\n",
    "spam_word_freq = pd.DataFrame(spam_words).value_counts()\n",
    "ham_word_freq = pd.DataFrame(ham_words).value_counts()\n",
    "\n",
    "word_freq = pd.DataFrame({\"#Spam\" : (spam_word_freq), \"#Ham\" : (ham_word_freq)}).fillna(0)\n",
    "\n",
    "word_freq = word_freq.reset_index()\n",
    "word_freq = word_freq.rename(columns = {0:\"Word\"})\n",
    "word_freq[word_freq[\"Word\"] == \"have\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1DBTOQpJbSqE"
   },
   "source": [
    "<h3>5. Visualise the Data</h3>\n",
    "<p>Let's use a Word Cloud library to visualise the most common words contained in spam messages.</p>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-05T00:46:34.247440Z",
     "start_time": "2023-05-05T00:46:33.619885Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "2mmdY8jWbkgS"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      4\u001b[0m wordcloud \u001b[38;5;241m=\u001b[39m WordCloud(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhite\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_font_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m70\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "wordcloud = WordCloud(background_color=\"white\", max_font_size=70)\n",
    "\n",
    "wordcloud.generate(' '.join(spam_words))\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bVpFmJ5kX_HG"
   },
   "source": [
    "<h3>6.  Calculate $P\\left(E\\middle| S\\right)$ and $P\\left(E|\\lnot S\\right)$</h3>\n",
    "<p>Next creating a new DataFrame named word_prob that gives the probability of each word being found in a spam and ham message.</p>\n",
    "<p>To calculate the probability of a word being spam we divide the number of times the word was found in spam by the total number of spam messages, likewise to calculate the probability of each word being found in a ham message we divide the number of times the word was found in a ham message by the total number of ham messages.</p>\n",
    "<p>If a word was not found in ham or spam it will cause problems later because the probability calculated will be zero. Therefore, we use a pseudocount k and estimate the probability of seeing the word. This is known as smoothing and results in the following formula when k = 0.5, for example.</p>\n",
    "<p>$P\\left(E\\middle| S\\right)$ = (number of spams containing the word + k) / (total number of spam messages + 2 * k).</p>\n",
    "<p>Likewise, for $P\\left(E|\\lnot S\\right)$.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-05T00:46:48.652476Z",
     "start_time": "2023-05-05T00:46:35.802164Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "U2kaXmftX_HH"
   },
   "outputs": [],
   "source": [
    "k = 0.5\n",
    "word_prob = word_freq.copy()\n",
    "for i in range (0,word_freq.shape[0]):\n",
    "    word = \" \" + word_freq.iat[i, 0] + \" \"\n",
    "    in_spam = 0\n",
    "    in_ham = 0\n",
    "    for j in spam_messages:\n",
    "        if word in j:\n",
    "            in_spam += 1\n",
    "    for m in ham_messages:\n",
    "        if word in m:\n",
    "            in_ham += 1\n",
    "            \n",
    "    word_prob.iat[i,1] = (in_spam + k) / (len(spam_messages) + (2 * k))\n",
    "    word_prob.iat[i,2] = (in_ham + k) / (len(ham_messages) + (2 * k))\n",
    "    \n",
    "word_prob = word_prob.rename(columns = {\"#Spam\" : \"P(E|S)\", \"#Ham\" : \"P(E|-S)\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iagF4brWA5QO"
   },
   "source": [
    "<h3>7. Checking the 'spamliness' of a single word</h3>\n",
    "<p>Now that we have trained the model, we will test the model.  Before we use the test_data, first let’s check how the model calculates the spamliness of a single word.  This is where we use the Bayes Theorem formula.  We have already calculated $P\\left(E\\middle| S\\right)$ and $P\\left(E|\\lnot S\\right)$, so we can just extract these values from the word_prob DataFrame.</p>\n",
    "<p>We need to decide on the prior values $P\\left(S\\right)$ and $P\\left(\\lnot S\\right)$, this is where we can experiment and tweak the model, in this example the prior value for spam was set to $0.4$ and the prior value for not spam or ham was set to $0.6$.</p>\n",
    "<h3>\n",
    "$P\\left(S\\middle|\\ E\\right)=\\frac{P\\left(E\\middle|\\ S\\right)P\\left(S\\right)}{P\\left(E\\middle|\\ S\\right)P\\left(S\\right)+P\\left(E|\\lnot S\\right)P\\left(\\lnot S\\right)}$\n",
    "</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-05T00:46:56.366805Z",
     "start_time": "2023-05-05T00:46:54.046347Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "NS-KBvz4-YhJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter a word : free\n",
      "Word = ['free'] \n",
      "P(E|S) = [0.2252650176678445]\n",
      "P(E|¬S) = [0.011756569847856155]\n",
      "P(S|E) = [0.9273986659825552]\n",
      "P(¬S|E) = [0.07260133401744484]\n"
     ]
    }
   ],
   "source": [
    "# a = P(S|E) \n",
    "# m = P(E|S)  \n",
    "# n = P(S) \n",
    "# p = P(E|-S) \n",
    "# q = P(-S) \n",
    "\n",
    "word = input(\"enter a word : \")\n",
    "n = 0.4\n",
    "q = 0.6\n",
    "m = word_prob[word_prob[\"Word\"] == word][\"P(E|S)\"].item()\n",
    "p = word_prob[word_prob[\"Word\"] == word][\"P(E|-S)\"].item()\n",
    "a = (m*n)/((m*n)+(p*q))\n",
    "print(f\"Word = ['{word}'] \\nP(E|S) = [{m}]\\nP(E|¬S) = [{p}]\\nP(S|E) = [{a}]\\nP(¬S|E) = [{1-a}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NiU4nDFrNW-7"
   },
   "source": [
    "<h3>8. Checking the 'spamliness' of several words</h3>\n",
    "<p>To check the spamliness of several words contained in a message we multiply the probabilities.  The model assumes the words appear as independent events hence the naïve Bayes.  In reality of course, words are not independent events, but the model still performs well.  So we use the assumption that the words appear independently, and hence we multiply probabilities, so\n",
    "$P(S\\,|\\, x_1,\\dots,x_n)\\approx \\frac{P(S)\\underset{i=1}{\\overset{n}{\\prod}}P(x_i | S)}{P(S)\\underset{i=1}{\\overset{n}{\\prod}}P(x_i | S)+P(\\neg S)\\underset{i=1}{\\overset{n}{\\prod}}P(x_i | \\neg S)}$\n",
    "\n",
    "Calculating the probability for each word in a message being spam, we might want to store the calculations in a list named prob_spam.  Likewise creating a list for each word not being spam.\n",
    "Then multiplying the probabilities and compare the results.  If the result of multiplying the probabilities for spam is greater than the result of multiplying the probabilities for not spam, then we assume the message as spam.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-05T00:47:02.792030Z",
     "start_time": "2023-05-05T00:46:58.347910Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "PGK027jUNlU7"
   },
   "outputs": [],
   "source": [
    "message = input(\"enter a message : \")\n",
    "n = 0.4\n",
    "q = 0.6\n",
    "prob_m = []\n",
    "prob_p = []\n",
    "\n",
    "words = message.split()\n",
    "for i in words:\n",
    "    if i in word_prob[\"Word\"].tolist():\n",
    "        word = i\n",
    "        m = word_prob[word_prob[\"Word\"] == word][\"P(E|S)\"].item()\n",
    "        p = word_prob[word_prob[\"Word\"] == word][\"P(E|-S)\"].item()\n",
    "        prob_m.append(m)\n",
    "        prob_p.append(p)\n",
    "        \n",
    "prodm = 1\n",
    "for num in prob_m:\n",
    "    prodm *= num \n",
    "prodm *= n\n",
    "\n",
    "prodp = 1\n",
    "for num in prob_p:\n",
    "    prodp *= num \n",
    "prodp *= q\n",
    "\n",
    "prob_spam = ((prodm)/(prodm + prodp))\n",
    "    \n",
    "prob_ham = ((prodp)/(prodm + prodp))\n",
    "\n",
    "if prob_ham >= prob_spam:\n",
    "    result = \"ham\"\n",
    "else:\n",
    "    result = \"spam\"\n",
    "    \n",
    "print(f\"Probability of message being spam is {round(prob_spam,2)}, ham is {round(prob_ham,2)}, As the probability of message being '{result}' is greater, hence the message is '{result}'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oE6CsYaev-w_"
   },
   "source": [
    "<h3>9. Avoiding floating point underflow</h3>\n",
    "<p>Our aim is to compare two probabilities $P(S|x_1,\\dots,x_n)$ with $P(\\neg S|x_1,\\dots,x_n),$ according to our model introduced in Section 8, both probabilities share a common denominator which does not affect comparison. Hence we will calculate numerators only, which are proportional to $P(S|x_1,\\dots,x_n)$ and $P(\\neg S|x_1,\\dots,x_n).$\n",
    "</p>\n",
    "\n",
    "<p>Multiplying a set of small probabilities could result in a floating-point error.  This is where the product becomes too small to be represented correctly.  To avoid this we can take the logarithm of the probabilities and add them.  \n",
    "\n",
    "To avoid multiplication of small numbers, we use the following property of $\\log(x):$</p>\n",
    "$$\n",
    "\\log(a\\cdot b)=\\log(a)+\\log(b)\n",
    "$$\n",
    "<p>i.e. the log of the product is equal to the sum of logs (so instead of multiplying small numbers we will add them):</p>\n",
    "$$\n",
    "P(S|x_1,x_2,\\dots,x_n)\\propto P(S)\\cdot P(x_1|S)\\cdot \\dots \\cdot P(x_n|S)$$\n",
    "<p>becomes</p>\n",
    "$$\\log(P(S|x_1,x_2,\\dots,x_n))\\propto \\log\\left(P(S)\\cdot P(x_1|S)\\cdot \\dots  P(x_n|S)\\right)=$$ $$\n",
    "\\log(P(S))+\\log(P(x_1|S))+\\dots+\\log(P(x_n|S))\n",
    "$$\n",
    "<p>So, to check spam or ham we just compare:</p>\n",
    "$$\n",
    "\\log(P(S))+\\log(P(x_1|S))+\\dots+\\log(P(x_n|S))\n",
    "$$\n",
    "<p>and </p>\n",
    "$$\n",
    "\\log(P(\\neg S))+\\log(P(x_1|\\neg S))+\\dots+\\log(P(x_n|\\neg S))\n",
    "$$\n",
    "\n",
    "\n",
    "Changing the equation so that logs are used.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-05T00:47:09.715081Z",
     "start_time": "2023-05-05T00:47:05.282152Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "4JYfU0vixQFw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter a message : here you go\n",
      "Probability of message being spam is -9.72, ham is -8.64, As the probability of message being 'ham', hence the message is 'ham'.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "message = input(\"enter a message : \")\n",
    "n = 0.4\n",
    "q = 0.6\n",
    "prob_spam = [n]\n",
    "prob_ham = [q]\n",
    "prod_ham = 0\n",
    "prod_spam = 0\n",
    "\n",
    "words = message.split()\n",
    "for i in words:\n",
    "    if i in word_prob[\"Word\"].tolist():\n",
    "        word = i\n",
    "        m = word_prob[word_prob[\"Word\"] == word][\"P(E|S)\"].item()\n",
    "        p = word_prob[word_prob[\"Word\"] == word][\"P(E|-S)\"].item()\n",
    "        prob_spam.append(m)\n",
    "        prob_ham.append(p)\n",
    "    \n",
    "for num in prob_ham:\n",
    "    prod_ham += np.log(num)\n",
    "    \n",
    "for num in prob_spam:\n",
    "    prod_spam += np.log(num)\n",
    "\n",
    "if prod_ham >= prod_spam:\n",
    "    result = \"ham\"\n",
    "else:\n",
    "    result = \"spam\"\n",
    "    \n",
    "print(f\"Probability of message being spam is {round(prod_spam,2)}, ham is {round(prod_ham,2)}, As the probability of message being '{result}', hence the message is '{result}'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZtiGi5X37b0G"
   },
   "source": [
    "<h3>10. Testing the Model</h3>\n",
    "<p>Now that we have tested the model using simple messages.  Let’s test the model using the messages from the test_set. Implementing counters that displays how our model has performed and calculate the accuracy of the model.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-05T00:48:09.085105Z",
     "start_time": "2023-05-05T00:47:11.344156Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "OOreoe6F7eri"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "result_data = []\n",
    "\n",
    "for w in test_data[\"Message\"].tolist():\n",
    "    message =  w\n",
    "    n = 0.4\n",
    "    q = 0.6\n",
    "    prob_spam = [n]\n",
    "    prob_ham = [q]\n",
    "    prod_ham = 0\n",
    "    prod_spam = 0\n",
    "\n",
    "    words = message.split()\n",
    "    for i in words:\n",
    "        if i in word_prob[\"Word\"].tolist():\n",
    "            word = i\n",
    "            m = word_prob[word_prob[\"Word\"] == word][\"P(E|S)\"].item()\n",
    "            p = word_prob[word_prob[\"Word\"] == word][\"P(E|-S)\"].item()\n",
    "            prob_spam.append(m)\n",
    "            prob_ham.append(p)\n",
    "\n",
    "    for num in prob_ham:\n",
    "        prod_ham += np.log(num) \n",
    "\n",
    "    for num in prob_spam:\n",
    "        prod_spam += np.log(num) \n",
    "\n",
    "    if prod_ham >= prod_spam:\n",
    "        result = \"ham\"\n",
    "    else:\n",
    "        result = \"spam\"\n",
    "    \n",
    "    result_data.append(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-05T00:49:10.898868Z",
     "start_time": "2023-05-05T00:49:10.790892Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match_ham = 1019 \n",
      "match_spam = 182 \n",
      "thought_ham_is_spam = 192 \n",
      "thought_spam_is_ham = 0 \n",
      "Accuracy = 0.8621679827709978\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4179</th>\n",
       "      <td>ham</td>\n",
       "      <td>swhrt how u deyhope ur ok tot about u 2daylove...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4180</th>\n",
       "      <td>ham</td>\n",
       "      <td>ok da i already planned i wil pick you</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4181</th>\n",
       "      <td>spam</td>\n",
       "      <td>urgent please call 0906346330 your abta compli...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4182</th>\n",
       "      <td>ham</td>\n",
       "      <td>sorry ill call later in meeting</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4183</th>\n",
       "      <td>ham</td>\n",
       "      <td>i just really need shit before tomorrow and i ...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4184</th>\n",
       "      <td>ham</td>\n",
       "      <td>im good have you registered to vote</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4185</th>\n",
       "      <td>ham</td>\n",
       "      <td>hmm ok ill stay for like an hour cos my eye is...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4186</th>\n",
       "      <td>ham</td>\n",
       "      <td>dear got bus directly to calicut</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4187</th>\n",
       "      <td>ham</td>\n",
       "      <td>mm umma ask vava also to come tell him can pla...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4188</th>\n",
       "      <td>ham</td>\n",
       "      <td>well the general price is  ltgt oz let me know...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4189</th>\n",
       "      <td>ham</td>\n",
       "      <td>sorry ill call later</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4190</th>\n",
       "      <td>ham</td>\n",
       "      <td>each moment in a dayhas its own valuemorning b...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4191</th>\n",
       "      <td>ham</td>\n",
       "      <td>ltgt  w jetton ave if you forgot</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4192</th>\n",
       "      <td>ham</td>\n",
       "      <td>ok im coming home now</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4193</th>\n",
       "      <td>ham</td>\n",
       "      <td>can not use foreign stamps in this country</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4194</th>\n",
       "      <td>spam</td>\n",
       "      <td>double mins and txts 4 6months free bluetooth ...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4195</th>\n",
       "      <td>ham</td>\n",
       "      <td>sorry its a lot of friendofafriend stuff im ju...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4196</th>\n",
       "      <td>spam</td>\n",
       "      <td>free for 1st week no1 nokia tone 4 ur mob ever...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4197</th>\n",
       "      <td>spam</td>\n",
       "      <td>want to funk up ur fone with a weekly new tone...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4198</th>\n",
       "      <td>spam</td>\n",
       "      <td>cmon babe make me horny turn me on txt me your...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Category                                            Message Predicted\n",
       "4179      ham  swhrt how u deyhope ur ok tot about u 2daylove...       ham\n",
       "4180      ham             ok da i already planned i wil pick you       ham\n",
       "4181     spam  urgent please call 0906346330 your abta compli...      spam\n",
       "4182      ham                    sorry ill call later in meeting       ham\n",
       "4183      ham  i just really need shit before tomorrow and i ...       ham\n",
       "4184      ham                im good have you registered to vote      spam\n",
       "4185      ham  hmm ok ill stay for like an hour cos my eye is...       ham\n",
       "4186      ham                   dear got bus directly to calicut       ham\n",
       "4187      ham  mm umma ask vava also to come tell him can pla...       ham\n",
       "4188      ham  well the general price is  ltgt oz let me know...       ham\n",
       "4189      ham                               sorry ill call later       ham\n",
       "4190      ham  each moment in a dayhas its own valuemorning b...      spam\n",
       "4191      ham                   ltgt  w jetton ave if you forgot       ham\n",
       "4192      ham                              ok im coming home now       ham\n",
       "4193      ham         can not use foreign stamps in this country      spam\n",
       "4194     spam  double mins and txts 4 6months free bluetooth ...      spam\n",
       "4195      ham  sorry its a lot of friendofafriend stuff im ju...       ham\n",
       "4196     spam  free for 1st week no1 nokia tone 4 ur mob ever...      spam\n",
       "4197     spam  want to funk up ur fone with a weekly new tone...      spam\n",
       "4198     spam  cmon babe make me horny turn me on txt me your...      spam"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match_ham = 0\n",
    "match_spam = 0\n",
    "thought_ham_is_spam = 0\n",
    "thought_spam_is_ham = 0\n",
    "\n",
    "predicted_data = pd.DataFrame()\n",
    "predicted_data = test_data.copy()\n",
    "predicted_data[\"Predicted\"] = result_data \n",
    "\n",
    "for i in range(predicted_data.shape[0]):\n",
    "    if predicted_data.iat[i,0] == predicted_data.iat[i,2]:\n",
    "        if predicted_data.iat[i,0] == \"ham\":\n",
    "            match_ham += 1\n",
    "        else:\n",
    "            match_spam += 1\n",
    "    \n",
    "    else:\n",
    "        if predicted_data.iat[i,0] ==\"ham\":\n",
    "            thought_ham_is_spam += 1\n",
    "        else:\n",
    "            thought_spam_is_ham += 1\n",
    "            \n",
    "print(f\"match_ham = {match_ham} \\nmatch_spam = {match_spam} \\nthought_ham_is_spam = {thought_ham_is_spam} \\nthought_spam_is_ham = {thought_spam_is_ham} \\nAccuracy = {(match_ham + match_spam)/(test_data.shape[0])}\")       \n",
    "predicted_data.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fwGqqNBzDh3y"
   },
   "source": [
    "<h3>11. Improvements</h3>\n",
    "<p>Giving some suggestions or recommendations on how the accuracy of the model could be improved.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-26T17:33:55.819477Z",
     "start_time": "2023-04-26T17:33:55.810345Z"
    }
   },
   "source": [
    "--- As we can see clearly thought_ham_is_spam = 192, which are misclassified hams. \n",
    "This is happening because our training data is limited, hence, if we have more data of ham and spam messages,\n",
    "We can better train our model.\n",
    "We can observe this by getting an accuracy increase of 2% when making training data as 90% of original data.\n",
    "\n",
    "--- We also know that, we only had one feature (Spam/Ham) to classify the messages. If we have more features like context of messages, sender details etc. to train the model, we could have classified the messages better."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "lab05_spam_filter_student.ipynb",
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
